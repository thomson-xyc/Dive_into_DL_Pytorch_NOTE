{"cells":[{"metadata":{"cell_type":"code","id":"4A49E1377A97427687A10EEFBE39BD99","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# 卷积神经网络基础\n对于图像数据的处理，如果用全连接层进行处理，我们需要把图像像素转换成向量作为全连接层的输入，这样变换后图像中相邻的像素可能在向量中相距较远，这样可能导致一些特征无法被识别；若输入的图像尺寸很大，会导致全连接层模型复杂，带来过高的存储开销。\n卷积神经网络(CNN)能很好的解决这两个问题。先看一张卷积神经网络的典型图片：\n![Image Name](https://cdn.kesci.com/upload/image/q5xqrdvg2r.jpg?imageView2/0/w/960/h/960)\n很明显，对于我这种小白来说，这个图片并不好理解，我们就先从一些基础看起。\n## 卷积\n卷积神经网络中肯定是存在卷积层的，那我们就来看看这个卷积是什么吧：\n## 二维互相关运算与卷积运算\n卷积神经网络的输入是图片，因此不会像全连接层那样变换图片的形状。计算机是通过计算来处理程序的，因此卷积神经网络将图片转换为矩阵，相当于二维张量，因此其运算是二维的。我们来看一下什么是互相关运算，先看下图：\n![Image Name](https://cdn.kesci.com/upload/image/q5xqsq2o3g.PNG?imageView2/0/w/960/h/960)\n在上图中，蓝色方块是一个3*3的矩阵，橙色方块是2*2的矩阵，两个矩阵做互相关运算就是：首先取蓝色矩阵左上角和橙色矩阵形状相同的矩阵，然后与橙色矩阵对应元素相乘，再将结果相加，得到一个数，作为结果矩阵（上图中绿色矩阵）的第一个元素；然后将橙色矩阵的形状在蓝色矩阵上向右滑动一个单位，继续进行对应元素相乘并相加，计算结果加到结果矩阵中，当橙色滑块的右边界到达蓝色矩阵右边界时，橙色滑块退回最左端，并向下滑动一个单位，然后继续操作，直到整个蓝色滑块全部计算完毕。这里面，蓝色矩阵是输入数据，橙色矩阵叫作卷积核。我在百度上找到下面这张图，很直观的表现了互相关运算的过程：\n![Image Name](https://cdn.kesci.com/upload/image/q5xqt9dx61.gif?imageView2/0/w/960/h/960)\n准确的说，卷积运算与互相关运算有些差别，但在卷积神经网络中卷积核是通过学习得到的，因此在卷积神经网络中卷积运算就是互相关运算。\n## 卷积核\n我们一般想到的神经网络是这个样子滴：\n![Image Name](https://cdn.kesci.com/upload/image/q5xqus7dq9.jpg?imageView2/0/w/960/h/960)\n我们知道，神经网络模型的学习过程就是寻找最优权重参数的过程，卷积神经网络本质也是这样，只不过它寻找的是最优的卷积核，而这个卷积核其实就是神经网络的权重参数。我们可以这样理解：对于上图这个一般的神经网络，我们就是通过输入预测对象的各属性值得到预测结果，而卷积神经网络就可以理解为将图像的像素作为输入数据的属性值，卷积核就是要学习的权重参数。\n## 特征图和感受野\n二维卷积层输出的矩阵可以看做是输入数据在空间维度上某一级的表征，我们称它为特征图（feature map），影响元素x的前向计算的所有可能输入区域（可能大于输入的实际尺寸）叫做的感受野（receptive field）。\n## 填充\n填充（padding）是指在输入高和宽的两侧填充元素（通常是0元素），通过填充，可以在给定定形的输入数据和卷积核下改变输出数据的形状。给输入数据填充0元素后做卷积运算如下图所示：\n![Image Name](https://cdn.kesci.com/upload/image/q5xqvugvl3.PNG?imageView2/0/w/960/h/960)\n## 步幅\n在卷积运算中，卷积核在输入数组上滑动，每次滑动的行数与列数叫作步幅（stride）。假设在高上步幅为3、宽上步幅为2，则卷积运算如下：\n![Image Name](https://cdn.kesci.com/upload/image/q5xqwg4mia.PNG?imageView2/0/w/960/h/960)\n多输入通道和多输出通道\n前面我们所看到的输入输出数据都是二维数据，即矩阵，但实际运用中我们见到的数据可能会有更高的维度，比如彩色图像除了宽高外还有（R,G,B）三色彩通道。输出数据亦可有多个通道。假设彩色图片宽高为w和h，则它可表示为一个3×w×h的多维数据，我们可以将其以如下图形表示：\n![Image Name](https://cdn.kesci.com/upload/image/q5xqx911dy.PNG?imageView2/0/w/960/h/960)\n有些资料中将通道个数叫作深度。\n## 池化\n在CNN中，通常会周期性的加入一个池化层，用来降低数据体的空间尺寸，缓解卷积层对位置的过度敏感性，通俗地讲，就是去除冗余信息。\n和卷积层相似，池化层每次对输入数据的一个固定形状窗口（又称池化窗口）中的元素计算输出，池化层直接计算池化窗口内元素的最大值或者平均值，该运算也分别叫做最大池化或平均池化。\n最大池化：\n![Image Name](https://cdn.kesci.com/upload/image/q5xqyeax22.PNG?imageView2/0/w/960/h/960)\n平均池化：\n![Image Name](https://cdn.kesci.com/upload/image/q5xqyxmmxt.PNG?imageView2/0/w/960/h/960)\n同样，池化也可以通过调整步幅大小来改变输出数据的形状，此操作与卷积层一样。\n在处理多通道输入数据时，池化层对每个输入通道分别池化，但不会像卷积层那样将各通道的结果按通道相加。因此，池化层的输出通道数与输入通道个数相等。"},{"metadata":{"id":"E686EBC127C64209B1CE554C5513C8BD","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"import torch \nimport torch.nn as nn\n\n# 卷积层\nX = torch.rand(4, 2, 3, 5)\nprint(X.shape)\n\nconv2d = nn.Conv2d(in_channels=2, out_channels=3, kernel_size=(3, 5), stride=1, padding=(1, 2))\nY = conv2d(X)\nprint('Y.shape: ', Y.shape)\nprint('weight.shape: ', conv2d.weight.shape)\nprint('bias.shape: ', conv2d.bias.shape)\n\n# 池化层\nX = torch.arange(32, dtype=torch.float32).view(1, 2, 4, 4)\npool2d = nn.MaxPool2d(kernel_size=3, padding=1, stride=(2, 1))\nY = pool2d(X)\nprint(X)\nprint(Y)","execution_count":null},{"metadata":{"id":"7958E971CD7349D0B7691615055B63FC","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# LeNet\nLeNet神经网络是1998年计算机科学家Yann LeCun等人提出的，这个网络充分考虑了图像的相关性。\n![Image Name](https://cdn.kesci.com/upload/image/q5xrfg91yf.PNG?imageView2/0/w/960/h/960)\nLeNet网络分为卷积层块和全连接层，其中卷积层块以一个卷积层后接一个平均池化层为基本单位，具体网络结构是这样的：\n输入：1×32×32的图片，单通道输入；\n第一卷积层：1×5×5的卷积核，通道为6，步幅为1，非全0填充；\n第一池化层： 2×2池化大小，步幅为2，全0填充；\n第二卷积层：6×5×5的卷积核，通道为16，步幅为1，非全0填充；\n第二池化层：2×2池化大小，步幅为2，全0填充；\n全连接层：将多维数据拉平，有3层，输出个数分别为120、80、10；\n输出：10分类。\n用流程图表示LeNet结构及特征提取过程如下：\n![Image Name](https://cdn.kesci.com/upload/image/q5xrg6c43v.PNG?imageView2/0/w/960/h/960)\n## pytorch实现如下"},{"metadata":{"id":"780A4704262A429D99B7859B5EF39223","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"import sys\nsys.path.append(\"/home/kesci/input\")\nimport d2lzh1981 as d2l\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport time\n\n#net\nclass Flatten(torch.nn.Module):  #展平操作\n    def forward(self, x):\n        return x.view(x.shape[0], -1)\n\nclass Reshape(torch.nn.Module): #将图像大小重定型\n    def forward(self, x):\n        return x.view(-1,1,28,28)      #(B x C x H x W)\n    \nnet = torch.nn.Sequential(     #Lelet                                                  \n    Reshape(),\n    nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, padding=2), #b*1*28*28  =>b*6*28*28\n    nn.Sigmoid(),                                                       \n    nn.AvgPool2d(kernel_size=2, stride=2),                              #b*6*28*28  =>b*6*14*14\n    nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5),           #b*6*14*14  =>b*16*10*10\n    nn.Sigmoid(),\n    nn.AvgPool2d(kernel_size=2, stride=2),                              #b*16*10*10  => b*16*5*5\n    Flatten(),                                                          #b*16*5*5   => b*400\n    nn.Linear(in_features=16*5*5, out_features=120),\n    nn.Sigmoid(),\n    nn.Linear(120, 84),\n    nn.Sigmoid(),\n    nn.Linear(84, 10)\n)","execution_count":1},{"metadata":{"id":"2865C6C9744E4A3CA95D5A9AFDE421A1","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"# 获取数据\nbatch_size = 256\ntrain_iter, test_iter = d2l.load_data_fashion_mnist(\n    batch_size=batch_size, root='/home/kesci/input/FashionMNIST2065')\nprint(len(train_iter))\n\n#数据展示\nimport matplotlib.pyplot as plt\ndef show_fashion_mnist(images, labels):\n    d2l.use_svg_display()\n    # 这里的_表示我们忽略（不使用）的变量\n    _, figs = plt.subplots(1, len(images), figsize=(12, 12))\n    for f, img, lbl in zip(figs, images, labels):\n        f.imshow(img.view((28, 28)).numpy())\n        f.set_title(lbl)\n        f.axes.get_xaxis().set_visible(False)\n        f.axes.get_yaxis().set_visible(False)\n    plt.show()\n\nfor Xdata,ylabel in train_iter:\n    break\nX, y = [], []\nfor i in range(10):\n    print(Xdata[i].shape,ylabel[i].numpy())\n    X.append(Xdata[i]) # 将第i个feature加到X中\n    y.append(ylabel[i].numpy()) # 将第i个label加到y中\nshow_fashion_mnist(X, y)","execution_count":null},{"metadata":{"id":"547443A87827495880273EFFFDEB68D5","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"# 使用GPU\ndef try_gpu():\n    \"\"\"If GPU is available, return torch.device as cuda:0; else return torch.device as cpu.\"\"\"\n    if torch.cuda.is_available():\n        device = torch.device('cuda:0')\n    else:\n        device = torch.device('cpu')\n    return device\n\ndevice = try_gpu()\ndevice","execution_count":null},{"metadata":{"id":"CBB8A7A3666145C1899834CB79FEB70C","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"#计算准确率\n'''\n(1). net.train()\n  启用 BatchNormalization 和 Dropout，将BatchNormalization和Dropout置为True\n(2). net.eval()\n不启用 BatchNormalization 和 Dropout，将BatchNormalization和Dropout置为False\n'''\n\ndef evaluate_accuracy(data_iter, net,device=torch.device('cpu')):\n    \"\"\"Evaluate accuracy of a model on the given data set.\"\"\"\n    acc_sum,n = torch.tensor([0],dtype=torch.float32,device=device),0\n    for X,y in data_iter:\n        # If device is the GPU, copy the data to the GPU.\n        X,y = X.to(device),y.to(device)\n        net.eval()\n        with torch.no_grad():\n            y = y.long()\n            acc_sum += torch.sum((torch.argmax(net(X), dim=1) == y))  #[[0.2 ,0.4 ,0.5 ,0.6 ,0.8] ,[ 0.1,0.2 ,0.4 ,0.3 ,0.1]] => [ 4 , 2 ]\n            n += y.shape[0]\n    return acc_sum.item()/n","execution_count":null},{"metadata":{"id":"AE702910043B42B3A0E9B1FEE29491D0","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"#训练函数\ndef train_ch5(net, train_iter, test_iter,criterion, num_epochs, batch_size, device,lr=None):\n    \"\"\"Train and evaluate a model with CPU or GPU.\"\"\"\n    print('training on', device)\n    net.to(device)\n    optimizer = optim.SGD(net.parameters(), lr=lr)\n    for epoch in range(num_epochs):\n        train_l_sum = torch.tensor([0.0],dtype=torch.float32,device=device)\n        train_acc_sum = torch.tensor([0.0],dtype=torch.float32,device=device)\n        n, start = 0, time.time()\n        for X, y in train_iter:\n            net.train()\n            \n            optimizer.zero_grad()\n            X,y = X.to(device),y.to(device) \n            y_hat = net(X)\n            loss = criterion(y_hat, y)\n            loss.backward()\n            optimizer.step()\n            \n            with torch.no_grad():\n                y = y.long()\n                train_l_sum += loss.float()\n                train_acc_sum += (torch.sum((torch.argmax(y_hat, dim=1) == y))).float()\n                n += y.shape[0]\n        test_acc = evaluate_accuracy(test_iter, net,device)\n        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f, '\n              'time %.1f sec'\n              % (epoch + 1, train_l_sum/n, train_acc_sum/n, test_acc,\n                 time.time() - start))","execution_count":null},{"metadata":{"id":"8390AA74F9D84B3C8528F11FF98A3B2B","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"# 训练模型\nlr, num_epochs = 0.9, 10\n\ndef init_weights(m):\n    if type(m) == nn.Linear or type(m) == nn.Conv2d:\n        torch.nn.init.xavier_uniform_(m.weight)\n\nnet.apply(init_weights)\nnet = net.to(device)\n\ncriterion = nn.CrossEntropyLoss()   #交叉熵描述了两个概率分布之间的距离，交叉熵越小说明两者之间越接近\ntrain_ch5(net, train_iter, test_iter, criterion,num_epochs, batch_size,device, lr)","execution_count":null},{"metadata":{"id":"E9CC38BC0534475E8CC9E28E2A12D32B","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"# 测试模型\nfor testdata,testlabe in test_iter:\n    testdata,testlabe = testdata.to(device),testlabe.to(device)\n    break\nprint(testdata.shape,testlabe.shape)\nnet.eval()\ny_pre = net(testdata)\nprint(torch.argmax(y_pre,dim=1)[:10])\nprint(testlabe[:10])","execution_count":null}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.3","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":0}