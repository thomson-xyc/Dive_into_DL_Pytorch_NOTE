{"cells":[{"metadata":{"cell_type":"code","id":"4B837497FA1846B281AF7DE9A2AE0383","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# 卷积神经网络进阶\nLeNet网络有如下一些缺点：\n1.神经网络计算复杂；\n2.还没有⼤量深⼊研究参数初始化和⾮凸优化算法等诸多领域。\n随着硬件技术和深度学习的发展，近年出现一些新的卷积神经网络，有着非常不错的效能。\n# 深度卷积神经网络（AlexNet）\n我们来看一张图：\n![Image Name](https://cdn.kesci.com/upload/image/q5xsfsjhq0.jpg?imageView2/0/w/960/h/960)\n是不是觉得这张图有点似曾相识？没错！就是基础篇开头那张图！这张图就是AlexNet的网络结构。2012年AlexNet横空出世，以绝对优势赢得了ImageNet2012图像识别挑战赛。\nAlexNet所用到的运算依然是基本的CNN所用的运算，因此并不难理解，我们来看看它相比LeNet改进了那些地方吧：\n首先，AlexNet包含8层变换，比LeNet在规模上更大一些，这8层变换中有5层卷积、2层全连接隐藏层以及1层全连接输出层。\n其次，LeNet中激活函数使用的是sigmoid函数，AlexNet将其换为ReLU函数，ReLU函数计算更简单，并且在不同参数初始化方法下可以使模型更容易训练。（原因：sigmoid函数在输出极度接近0或1时，这些区域的梯度几乎为0，从而造成反向传播无法继续更新部分模型参数；ReLU函数则在正区间梯度恒为1，因此，若模型参数初始化不当，sigmoid函数可能在正区间得到几乎为0的梯度，使得模型无法得到有效训练。）\n还有，AlexNet加入了丢弃法，以此来控制全连接层的模型复杂度。\n最后，AlexNet引入了大量的诸如翻转、剪裁和颜色变化等图像增广，从而进一步扩大数据集来环节过拟合。\n根据上图，我们可以看见网络分为上下两部分，这两部分分别在两个GPU上计算，我们来简单看一下AlexNet具体结构：\n第一层：输入224×224×3的图像数据，卷积核大小为11×11×3，有96个通道，上下各48个通道，步长（就是步幅）为4，不进行边缘填充；然后进行局部响应归一化(Local Response Normalized)，紧跟着进行窗口大小为3×3、步长为2且不填充的最大池化操作；\n第二层：卷积核大小5×5×48，通道数为256，上下各128个，步长为1，填充2层，通过LRN，做窗口形状为3×3，步长为2的最大池化；\n第三层：卷积核大小为3×3×256，通道数为384，1层0填充；无LRN和池化；\n第四层：卷积核大小为3×3，通道数为384，1层0填充；无LRN和池化；\n第五层：卷积核大小为3×3，通道数为256,1层0填充，然后进行最大池化，窗口大小为3×3，步长为2；\n全连接层：剩下三层为全连接层，每层有4096个神经元结点，输出数据大小为1000。\n## pytorch实现如下："},{"metadata":{"id":"082358112F9F4CD88DD6D29DDF7BB251","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"import time\nimport torch\nfrom torch import nn, optim\nimport torchvision\nimport numpy as np\nimport sys\nsys.path.append(\"/home/kesci/input/\") \nimport d2lzh1981 as d2l\nimport os\nimport torch.nn.functional as F\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# 搭建AlexNet网络\nclass AlexNet(nn.Module):\n    def __init__(self):\n        super(AlexNet, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(1, 96, 11, 4), # in_channels, out_channels, kernel_size, stride, padding\n            nn.ReLU(),\n            nn.MaxPool2d(3, 2), # kernel_size, stride\n            # 减小卷积窗口，使用填充为2来使得输入与输出的高和宽一致，且增大输出通道数\n            nn.Conv2d(96, 256, 5, 1, 2),\n            nn.ReLU(),\n            nn.MaxPool2d(3, 2),\n            # 连续3个卷积层，且使用更小的卷积窗口。除了最后的卷积层外，进一步增大了输出通道数。\n            # 前两个卷积层后不使用池化层来减小输入的高和宽\n            nn.Conv2d(256, 384, 3, 1, 1),\n            nn.ReLU(),\n            nn.Conv2d(384, 384, 3, 1, 1),\n            nn.ReLU(),\n            nn.Conv2d(384, 256, 3, 1, 1),\n            nn.ReLU(),\n            nn.MaxPool2d(3, 2)\n        )\n         # 这里全连接层的输出个数比LeNet中的大数倍。使用丢弃层来缓解过拟合\n        self.fc = nn.Sequential(\n            nn.Linear(256*5*5, 4096),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            #由于使用CPU镜像，精简网络，若为GPU镜像可添加该层\n            #nn.Linear(4096, 4096),\n            #nn.ReLU(),\n            #nn.Dropout(0.5),\n\n            # 输出层。由于这里使用Fashion-MNIST，所以用类别数为10，而非论文中的1000\n            nn.Linear(4096, 10),\n        )\n\n    def forward(self, img):\n\n        feature = self.conv(img)\n        output = self.fc(feature.view(img.shape[0], -1))\n        return output","execution_count":null},{"metadata":{"id":"E00DFCC58190466D855C7EEE120D3A57","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"# 载入数据集\ndef load_data_fashion_mnist(batch_size, resize=None, root='/home/kesci/input/FashionMNIST2065'):\n    \"\"\"Download the fashion mnist dataset and then load into memory.\"\"\"\n    trans = []\n    if resize:\n        trans.append(torchvision.transforms.Resize(size=resize))\n    trans.append(torchvision.transforms.ToTensor())\n    \n    transform = torchvision.transforms.Compose(trans)\n    mnist_train = torchvision.datasets.FashionMNIST(root=root, train=True, download=True, transform=transform)\n    mnist_test = torchvision.datasets.FashionMNIST(root=root, train=False, download=True, transform=transform)\n\n    train_iter = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, shuffle=True, num_workers=2)\n    test_iter = torch.utils.data.DataLoader(mnist_test, batch_size=batch_size, shuffle=False, num_workers=2)\n\n    return train_iter, test_iter\n\n#batchsize=128\nbatch_size = 16\n# 如出现“out of memory”的报错信息，可减小batch_size或resize\ntrain_iter, test_iter = load_data_fashion_mnist(batch_size,224)\nfor X, Y in train_iter:\n    print('X =', X.shape,\n        '\\nY =', Y.type(torch.int32))\n    break","execution_count":null},{"metadata":{"id":"6586780E52E043F286661C7E037E76BC","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"# 训练\nlr, num_epochs = 0.001, 3\noptimizer = torch.optim.Adam(net.parameters(), lr=lr)\nd2l.train_ch5(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs)","execution_count":null},{"metadata":{"id":"33A1D0DD346844648814D21C4B8287F5","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# 使用重复元素的网络（VGG）\nVGG这个名来源于论文作者所在的实验室Visual Geometry Group。VGG提出了可以通过重复使用简单的基础块来构建深度模型的思路。\n与AlexNet、LeNet一样，VGG网络也是由卷积层模块后接全连接层模块构成。卷积层模块连续使用数个相同的填充为1、窗口形状为3×3的卷积层后街上一个步长为2、窗口形状为2×2的最大池化层。卷积层保持输入的高和宽不变，池化层则对其减半。\n## 实现如下："},{"metadata":{"id":"4C27723E37AB444C8F922DFC4F288C47","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"# 搭建网络结构\ndef vgg_block(num_convs, in_channels, out_channels): #卷积层个数，输入通道数，输出通道数\n    blk = []\n    for i in range(num_convs):\n        if i == 0:\n            blk.append(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1))\n        else:\n            blk.append(nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1))\n        blk.append(nn.ReLU())\n    blk.append(nn.MaxPool2d(kernel_size=2, stride=2)) # 这里会使宽高减半\n    return nn.Sequential(*blk)\n    \nconv_arch = ((1, 1, 64), (1, 64, 128), (2, 128, 256), (2, 256, 512), (2, 512, 512))\n# 经过5个vgg_block, 宽高会减半5次, 变成 224/32 = 7\nfc_features = 512 * 7 * 7 # c * w * h\nfc_hidden_units = 4096 # 任意\n\ndef vgg(conv_arch, fc_features, fc_hidden_units=4096):\n    net = nn.Sequential()\n    # 卷积层部分\n    for i, (num_convs, in_channels, out_channels) in enumerate(conv_arch):\n        # 每经过一个vgg_block都会使宽高减半\n        net.add_module(\"vgg_block_\" + str(i+1), vgg_block(num_convs, in_channels, out_channels))\n    # 全连接层部分\n    net.add_module(\"fc\", nn.Sequential(d2l.FlattenLayer(),\n                                 nn.Linear(fc_features, fc_hidden_units),\n                                 nn.ReLU(),\n                                 nn.Dropout(0.5),\n                                 nn.Linear(fc_hidden_units, fc_hidden_units),\n                                 nn.ReLU(),\n                                 nn.Dropout(0.5),\n                                 nn.Linear(fc_hidden_units, 10)\n                                ))\n    return net","execution_count":null},{"metadata":{"id":"B6F78EF64E514A8684FD57C0713E88CD","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"net = vgg(conv_arch, fc_features, fc_hidden_units)\nX = torch.rand(1, 1, 224, 224)\n\n# named_children获取一级子模块及其名字(named_modules会返回所有子模块,包括子模块的子模块)\nfor name, blk in net.named_children(): \n    X = blk(X)\n    print(name, 'output shape: ', X.shape)\n\nratio = 8\nsmall_conv_arch = [(1, 1, 64//ratio), (1, 64//ratio, 128//ratio), (2, 128//ratio, 256//ratio), \n                   (2, 256//ratio, 512//ratio), (2, 512//ratio, 512//ratio)]\nnet = vgg(small_conv_arch, fc_features // ratio, fc_hidden_units // ratio)\nprint(net)","execution_count":null},{"metadata":{"id":"E269863964644B6D8CAB7D6CDA78C17E","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"# 训练\nbatchsize=16\n#batch_size = 64\n# 如出现“out of memory”的报错信息，可减小batch_size或resize\n# train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=224)\n\nlr, num_epochs = 0.001, 5\noptimizer = torch.optim.Adam(net.parameters(), lr=lr)\nd2l.train_ch5(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs)","execution_count":null},{"metadata":{"id":"872290A1137942C783D669681AE3A7F6","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# ⽹络中的⽹络（NIN）\n与前面的几个网络不同，NIN给出了另一种思路：串联多个由卷积层和“全连接”层构成的小网络构建一个深层网络。其局部结构图如下：\n![Image Name](https://cdn.kesci.com/upload/image/q5xsxifsxt.PNG?imageView2/0/w/960/h/960)\n这里1×1卷积核可以通过控制卷积核的数量达到通道数的放缩，同时可以增加网络的非线性。NIN去除了容易造成过拟合的全连接输出层，而是将其替换成输出通道数等于标签类别数的NIN块和全局平均池化层。\n## 实现如下："},{"metadata":{"id":"7D12311F86D34A038223FCEE248C9749","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"# 搭建网络\ndef nin_block(in_channels, out_channels, kernel_size, stride, padding):\n    blk = nn.Sequential(nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding),\n                        nn.ReLU(),\n                        nn.Conv2d(out_channels, out_channels, kernel_size=1),\n                        nn.ReLU(),\n                        nn.Conv2d(out_channels, out_channels, kernel_size=1),\n                        nn.ReLU())\n    return blk\n\nclass GlobalAvgPool2d(nn.Module):\n    # 全局平均池化层可通过将池化窗口形状设置成输入的高和宽实现\n    def __init__(self):\n        super(GlobalAvgPool2d, self).__init__()\n    def forward(self, x):\n        return F.avg_pool2d(x, kernel_size=x.size()[2:])\n\nnet = nn.Sequential(\n    nin_block(1, 96, kernel_size=11, stride=4, padding=0),\n    nn.MaxPool2d(kernel_size=3, stride=2),\n    nin_block(96, 256, kernel_size=5, stride=1, padding=2),\n    nn.MaxPool2d(kernel_size=3, stride=2),\n    nin_block(256, 384, kernel_size=3, stride=1, padding=1),\n    nn.MaxPool2d(kernel_size=3, stride=2), \n    nn.Dropout(0.5),\n    # 标签类别数是10\n    nin_block(384, 10, kernel_size=3, stride=1, padding=1),\n    GlobalAvgPool2d(), \n    # 将四维的输出转成二维的输出，其形状为(批量大小, 10)\n    d2l.FlattenLayer())","execution_count":null},{"metadata":{"id":"F6A98CEE5B934ECE881D87846F7DD644","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"X = torch.rand(1, 1, 224, 224)\nfor name, blk in net.named_children(): \n    X = blk(X)\n    print(name, 'output shape: ', X.shape)\n\nbatch_size = 128\n# 如出现“out of memory”的报错信息，可减小batch_size或resize\n#train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=224)\n\n# 训练\nlr, num_epochs = 0.002, 5\noptimizer = torch.optim.Adam(net.parameters(), lr=lr)\nd2l.train_ch5(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs)","execution_count":null},{"metadata":{"id":"401DE5D71C7441AC8E882C1B814BBC61","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"3 含并行连结的网络GoogLeNet\n2014年的ImageNet挑战赛(ILSVRC14)中，GoogleNet和VGG分别获得第一名与第二名，根据前面讨论的VGG，我们可以看出VGG继承了LeNet及AlexNet的一些框架结构，而GoogleNet则大不相同。我们就来看看它是什么样的：\ngoogleNet中的基础块叫作Inception块，这个基础块在结构上比NIN块更加复杂，其结构如下：\n![Image Name](https://cdn.kesci.com/upload/image/q5xt2qf52o.PNG?imageView2/0/w/960/h/960)\n其整体结构如下：\n![Image Name](https://cdn.kesci.com/upload/image/q5xt3bjy9b.PNG?imageView2/0/w/960/h/960)\n## 实现如下："},{"metadata":{"id":"E7EA6B15A0BF47798E880135611590A7","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"# 定义Inception块\nclass Inception(nn.Module):\n    # c1 - c4为每条线路里的层的输出通道数\n    def __init__(self, in_c, c1, c2, c3, c4):\n        super(Inception, self).__init__()\n        # 线路1，单1 x 1卷积层\n        self.p1_1 = nn.Conv2d(in_c, c1, kernel_size=1)\n        # 线路2，1 x 1卷积层后接3 x 3卷积层\n        self.p2_1 = nn.Conv2d(in_c, c2[0], kernel_size=1)\n        self.p2_2 = nn.Conv2d(c2[0], c2[1], kernel_size=3, padding=1)\n        # 线路3，1 x 1卷积层后接5 x 5卷积层\n        self.p3_1 = nn.Conv2d(in_c, c3[0], kernel_size=1)\n        self.p3_2 = nn.Conv2d(c3[0], c3[1], kernel_size=5, padding=2)\n        # 线路4，3 x 3最大池化层后接1 x 1卷积层\n        self.p4_1 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n        self.p4_2 = nn.Conv2d(in_c, c4, kernel_size=1)\n\n    def forward(self, x):\n        p1 = F.relu(self.p1_1(x))\n        p2 = F.relu(self.p2_2(F.relu(self.p2_1(x))))\n        p3 = F.relu(self.p3_2(F.relu(self.p3_1(x))))\n        p4 = F.relu(self.p4_2(self.p4_1(x)))\n        return torch.cat((p1, p2, p3, p4), dim=1)  # 在通道维上连结输出\n\n# 搭建网络\nb1 = nn.Sequential(nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3),\n                   nn.ReLU(),\n                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n\nb2 = nn.Sequential(nn.Conv2d(64, 64, kernel_size=1),\n                   nn.Conv2d(64, 192, kernel_size=3, padding=1),\n                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n\nb3 = nn.Sequential(Inception(192, 64, (96, 128), (16, 32), 32),\n                   Inception(256, 128, (128, 192), (32, 96), 64),\n                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n\nb4 = nn.Sequential(Inception(480, 192, (96, 208), (16, 48), 64),\n                   Inception(512, 160, (112, 224), (24, 64), 64),\n                   Inception(512, 128, (128, 256), (24, 64), 64),\n                   Inception(512, 112, (144, 288), (32, 64), 64),\n                   Inception(528, 256, (160, 320), (32, 128), 128),\n                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n\nb5 = nn.Sequential(Inception(832, 256, (160, 320), (32, 128), 128),\n                   Inception(832, 384, (192, 384), (48, 128), 128),\n                   d2l.GlobalAvgPool2d())\n\nnet = nn.Sequential(b1, b2, b3, b4, b5, \n                    d2l.FlattenLayer(), nn.Linear(1024, 10))\n\nnet = nn.Sequential(b1, b2, b3, b4, b5, d2l.FlattenLayer(), nn.Linear(1024, 10))\n\nX = torch.rand(1, 1, 96, 96)\n\nfor blk in net.children(): \n    X = blk(X)\n    print('output shape: ', X.shape)\n\n# 训练\n#batchsize=128\nbatch_size = 16\n# 如出现“out of memory”的报错信息，可减小batch_size或resize\n#train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=96)\n\nlr, num_epochs = 0.001, 5\noptimizer = torch.optim.Adam(net.parameters(), lr=lr)\nd2l.train_ch5(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs)","execution_count":null},{"metadata":{"id":"26FF2C59CE78426299BEF98FFE47A634","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"","execution_count":null}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.3","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":0}