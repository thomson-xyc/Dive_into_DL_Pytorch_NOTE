{"cells":[{"metadata":{"cell_type":"code","id":"17749A1AD1EB4042912286ECD70DF662","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# 注意力机制\n（P.S.这几节没看明白，就简单的笔记一下。。）\n在seq2seq模型中，解码器只能隐式地从编码器的最终状态中选择相应的信息。然而，注意力机制可以将这种选择过程显式地建模。\n![Image Name](https://cdn.kesci.com/upload/image/q5xyj4p2lk.PNG?imageView2/0/w/960/h/960)\n"},{"metadata":{"id":"4A090AFEED2A4ABF8466A69AE774B1FC","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"import math\nimport torch \nimport torch.nn as nn\nimport os\ndef file_name_walk(file_dir):\n    for root, dirs, files in os.walk(file_dir):\n#         print(\"root\", root)  # 当前目录路径\n         print(\"dirs\", dirs)  # 当前路径下所有子目录\n         print(\"files\", files)  # 当前路径下所有非目录子文件\n\nfile_name_walk(\"/home/kesci/input/fraeng6506\")","execution_count":null},{"metadata":{"id":"86A55DCC3F59463283F03842E9A8E2FF","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"# Softmax屏蔽\ndef SequenceMask(X, X_len,value=-1e6):\n    maxlen = X.size(1)\n    #print(X.size(),torch.arange((maxlen),dtype=torch.float)[None, :],'\\n',X_len[:, None] )\n    mask = torch.arange((maxlen),dtype=torch.float)[None, :] >= X_len[:, None]   \n    #print(mask)\n    X[mask]=value\n    return X\n\ndef masked_softmax(X, valid_length):\n    # X: 3-D tensor, valid_length: 1-D or 2-D tensor\n    softmax = nn.Softmax(dim=-1)\n    if valid_length is None:\n        return softmax(X)\n    else:\n        shape = X.shape\n        if valid_length.dim() == 1:\n            try:\n                valid_length = torch.FloatTensor(valid_length.numpy().repeat(shape[1], axis=0))#[2,2,3,3]\n            except:\n                valid_length = torch.FloatTensor(valid_length.cpu().numpy().repeat(shape[1], axis=0))#[2,2,3,3]\n        else:\n            valid_length = valid_length.reshape((-1,))\n        # fill masked elements with a large negative, whose exp is 0\n        X = SequenceMask(X.reshape((-1, shape[-1])), valid_length)\n \n        return softmax(X).reshape(shape)\n\nmasked_softmax(torch.rand((2,2,4),dtype=torch.float), torch.FloatTensor([2,3]))","execution_count":null},{"metadata":{"id":"6BB803C24F32468FBE5EE680D99381D4","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"## 点积注意力\n![Image Name](https://cdn.kesci.com/upload/image/q5xym6nfoi.PNG?imageView2/0/w/960/h/960)"},{"metadata":{"id":"99ED275E231E4F8EB978D6FA56368E90","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"# 点积注意力\nclass DotProductAttention(nn.Module): \n    def __init__(self, dropout, **kwargs):\n        super(DotProductAttention, self).__init__(**kwargs)\n        self.dropout = nn.Dropout(dropout)\n\n    # query: (batch_size, #queries, d)\n    # key: (batch_size, #kv_pairs, d)\n    # value: (batch_size, #kv_pairs, dim_v)\n    # valid_length: either (batch_size, ) or (batch_size, xx)\n    def forward(self, query, key, value, valid_length=None):\n        d = query.shape[-1]\n        # set transpose_b=True to swap the last two dimensions of key\n        \n        scores = torch.bmm(query, key.transpose(1,2)) / math.sqrt(d)\n        attention_weights = self.dropout(masked_softmax(scores, valid_length))\n        print(\"attention_weight\\n\",attention_weights)\n        return torch.bmm(attention_weights, value)\n\n# 测试\natten = DotProductAttention(dropout=0)\n\nkeys = torch.ones((2,10,2),dtype=torch.float)\nvalues = torch.arange((40), dtype=torch.float).view(1,10,4).repeat(2,1,1)\natten(torch.ones((2,1,2),dtype=torch.float), keys, values, torch.FloatTensor([2, 6]))","execution_count":null},{"metadata":{"id":"A090287F0BEB4E6AAB02D3E539F63372","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"## 多层感知机注意力\n![Image Name](https://cdn.kesci.com/upload/image/q5xyps99am.PNG?imageView2/0/w/960/h/960)"},{"metadata":{"id":"2913D552E59C4DDA87FEBE6E221067A1","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"# 多层感知机注意力\nclass MLPAttention(nn.Module):  \n    def __init__(self, units,ipt_dim,dropout, **kwargs):\n        super(MLPAttention, self).__init__(**kwargs)\n        # Use flatten=True to keep query's and key's 3-D shapes.\n        self.W_k = nn.Linear(ipt_dim, units, bias=False)\n        self.W_q = nn.Linear(ipt_dim, units, bias=False)\n        self.v = nn.Linear(units, 1, bias=False)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, query, key, value, valid_length):\n        query, key = self.W_k(query), self.W_q(key)\n        #print(\"size\",query.size(),key.size())\n        # expand query to (batch_size, #querys, 1, units), and key to\n        # (batch_size, 1, #kv_pairs, units). Then plus them with broadcast.\n        features = query.unsqueeze(2) + key.unsqueeze(1)\n        #print(\"features:\",features.size())  #--------------开启\n        scores = self.v(features).squeeze(-1) \n        attention_weights = self.dropout(masked_softmax(scores, valid_length))\n        return torch.bmm(attention_weights, value)\n\n# 测试\natten = MLPAttention(ipt_dim=2,units = 8, dropout=0)\natten(torch.ones((2,1,2), dtype = torch.float), keys, values, torch.FloatTensor([2, 6]))","execution_count":null},{"metadata":{"id":"FDDF3E4F37E34D9C9BA541DC608F17AC","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# 引入注意力机制的Seq2seq模型"},{"metadata":{"id":"D38F845D4273479584A4B6CC62566109","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"import sys\nsys.path.append('/home/kesci/input/d2len9900')\nimport d2l\n\n# 解码器\nclass Seq2SeqAttentionDecoder(d2l.Decoder):\n    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,\n                 dropout=0, **kwargs):\n        super(Seq2SeqAttentionDecoder, self).__init__(**kwargs)\n        self.attention_cell = MLPAttention(num_hiddens,num_hiddens, dropout)\n        self.embedding = nn.Embedding(vocab_size, embed_size)\n        self.rnn = nn.LSTM(embed_size+ num_hiddens,num_hiddens, num_layers, dropout=dropout)\n        self.dense = nn.Linear(num_hiddens,vocab_size)\n\n    def init_state(self, enc_outputs, enc_valid_len, *args):\n        outputs, hidden_state = enc_outputs\n#         print(\"first:\",outputs.size(),hidden_state[0].size(),hidden_state[1].size())\n        # Transpose outputs to (batch_size, seq_len, hidden_size)\n        return (outputs.permute(1,0,-1), hidden_state, enc_valid_len)\n        #outputs.swapaxes(0, 1)\n        \n    def forward(self, X, state):\n        enc_outputs, hidden_state, enc_valid_len = state\n        #(\"X.size\",X.size())\n        X = self.embedding(X).transpose(0,1)\n#         print(\"Xembeding.size2\",X.size())\n        outputs = []\n        for l, x in enumerate(X):\n#             print(f\"\\n{l}-th token\")\n#             print(\"x.first.size()\",x.size())\n            # query shape: (batch_size, 1, hidden_size)\n            # select hidden state of the last rnn layer as query\n            query = hidden_state[0][-1].unsqueeze(1) # np.expand_dims(hidden_state[0][-1], axis=1)\n            # context has same shape as query\n#             print(\"query enc_outputs, enc_outputs:\\n\",query.size(), enc_outputs.size(), enc_outputs.size())\n            context = self.attention_cell(query, enc_outputs, enc_outputs, enc_valid_len)\n            # Concatenate on the feature dimension\n#             print(\"context.size:\",context.size())\n            x = torch.cat((context, x.unsqueeze(1)), dim=-1)\n            # Reshape x to (1, batch_size, embed_size+hidden_size)\n#             print(\"rnn\",x.size(), len(hidden_state))\n            out, hidden_state = self.rnn(x.transpose(0,1), hidden_state)\n            outputs.append(out)\n        outputs = self.dense(torch.cat(outputs, dim=0))\n        return outputs.transpose(0, 1), [enc_outputs, hidden_state,\n                                        enc_valid_len]\n\n# 测试seq2seq\nencoder = d2l.Seq2SeqEncoder(vocab_size=10, embed_size=8,\n                            num_hiddens=16, num_layers=2)\n# encoder.initialize()\ndecoder = Seq2SeqAttentionDecoder(vocab_size=10, embed_size=8,\n                                  num_hiddens=16, num_layers=2)\nX = torch.zeros((4, 7),dtype=torch.long)\nprint(\"batch size=4\\nseq_length=7\\nhidden dim=16\\nnum_layers=2\\n\")\nprint('encoder output size:', encoder(X)[0].size())\nprint('encoder hidden size:', encoder(X)[1][0].size())\nprint('encoder memory size:', encoder(X)[1][1].size())\nstate = decoder.init_state(encoder(X), None)\nout, state = decoder(X, state)\nout.shape, len(state), state[0].shape, len(state[1]), state[1][0].shape","execution_count":null},{"metadata":{"id":"C25FD23ADB7B4E6C80364DFE29A96C26","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"","execution_count":null}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.3","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":0}