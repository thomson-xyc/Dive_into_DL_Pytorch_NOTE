{"cells":[{"metadata":{"cell_type":"code","id":"FC38E7E6CB0D49E28E8333C57FBFFB1B","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# Transformer\n（同上节，尚未掌握，先过一遍概念，简单笔记一下）\nTransformer模型是为了整合CNN和RNN的优势而设计的，其设计使用了注意力机制。Transformer模型利用attention机制实现了并行化捕捉序列依赖，并且同时处理序列的每个位置的tokens，这样使得Transformer模型在性能优异的同时大大减少了训练时间。\nTransformer模型架构如下：\n![Image Name](https://cdn.kesci.com/upload/image/q5y0cwgkew.PNG?imageView2/0/w/960/h/960)"},{"metadata":{"id":"FCC72A84DADA4C0D86AAF78161D4FC86","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"import os\nimport math\nimport numpy as np\nimport torch \nimport torch.nn as nn\nimport torch.nn.functional as F\nimport sys\nsys.path.append('/home/kesci/input/d2len9900')\nimport d2l","execution_count":null},{"metadata":{"id":"F6D6206C53FA43F284ADEF7508FB5195","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"code","outputs":[],"source":"def SequenceMask(X, X_len,value=-1e6):\n    maxlen = X.size(1)\n    X_len = X_len.to(X.device)\n    #print(X.size(),torch.arange((maxlen),dtype=torch.float)[None, :],'\\n',X_len[:, None] )\n    mask = torch.arange((maxlen), dtype=torch.float, device=X.device)\n    mask = mask[None, :] < X_len[:, None]\n    #print(mask)\n    X[~mask]=value\n    return X\n\ndef masked_softmax(X, valid_length):\n    # X: 3-D tensor, valid_length: 1-D or 2-D tensor\n    softmax = nn.Softmax(dim=-1)\n    if valid_length is None:\n        return softmax(X)\n    else:\n        shape = X.shape\n        if valid_length.dim() == 1:\n            try:\n                valid_length = torch.FloatTensor(valid_length.numpy().repeat(shape[1], axis=0))#[2,2,3,3]\n            except:\n                valid_length = torch.FloatTensor(valid_length.cpu().numpy().repeat(shape[1], axis=0))#[2,2,3,3]\n        else:\n            valid_length = valid_length.reshape((-1,))\n        # fill masked elements with a large negative, whose exp is 0\n        X = SequenceMask(X.reshape((-1, shape[-1])), valid_length)\n \n        return softmax(X).reshape(shape)\n\n# Save to the d2l package.\nclass DotProductAttention(nn.Module): \n    def __init__(self, dropout, **kwargs):\n        super(DotProductAttention, self).__init__(**kwargs)\n        self.dropout = nn.Dropout(dropout)\n\n    # query: (batch_size, #queries, d)\n    # key: (batch_size, #kv_pairs, d)\n    # value: (batch_size, #kv_pairs, dim_v)\n    # valid_length: either (batch_size, ) or (batch_size, xx)\n    def forward(self, query, key, value, valid_length=None):\n        d = query.shape[-1]\n        # set transpose_b=True to swap the last two dimensions of key\n        scores = torch.bmm(query, key.transpose(1,2)) / math.sqrt(d)\n        attention_weights = self.dropout(masked_softmax(scores, valid_length))\n        return torch.bmm(attention_weights, value)","execution_count":null},{"metadata":{"id":"D506EC815BC0497180CE9D3DB6F80AC0","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"## 自注意力\n自注意力模型是一个正规的注意力模型，序列的每一个元素对应的key，value，query是完全一致的。自注意力结构如下：\n![Image Name](https://cdn.kesci.com/upload/image/q5y0n5e2xk.PNG?imageView2/0/w/960/h/960)\n## 多注意力机层\n多头注意力层包含h个并行的自注意力层，每一个这种层被成为一个head。对每个头来说，在进行注意力计算之前，我们会将query、key和value用三个现行层进行映射，这h个注意力头的输出将会被拼接之后输入最后一个线性层进行整合。\n![Image Name](https://cdn.kesci.com/upload/image/q5y0pmdisu.PNG?imageView2/0/w/960/h/960)\n![Image Name](https://cdn.kesci.com/upload/image/q5y0qskwdv.PNG?imageView2/0/w/960/h/960)"},{"metadata":{"id":"E66DA0E376AD492E80AADA4CC7E509FE","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"class MultiHeadAttention(nn.Module):\n    def __init__(self, input_size, hidden_size, num_heads, dropout, **kwargs):\n        super(MultiHeadAttention, self).__init__(**kwargs)\n        self.num_heads = num_heads\n        self.attention = DotProductAttention(dropout)\n        self.W_q = nn.Linear(input_size, hidden_size, bias=False)\n        self.W_k = nn.Linear(input_size, hidden_size, bias=False)\n        self.W_v = nn.Linear(input_size, hidden_size, bias=False)\n        self.W_o = nn.Linear(hidden_size, hidden_size, bias=False)\n    \n    def forward(self, query, key, value, valid_length):\n        # query, key, and value shape: (batch_size, seq_len, dim),\n        # where seq_len is the length of input sequence\n        # valid_length shape is either (batch_size, )\n        # or (batch_size, seq_len).\n\n        # Project and transpose query, key, and value from\n        # (batch_size, seq_len, hidden_size * num_heads) to\n        # (batch_size * num_heads, seq_len, hidden_size).\n        \n        query = transpose_qkv(self.W_q(query), self.num_heads)\n        key = transpose_qkv(self.W_k(key), self.num_heads)\n        value = transpose_qkv(self.W_v(value), self.num_heads)\n        \n        if valid_length is not None:\n            # Copy valid_length by num_heads times\n            device = valid_length.device\n            valid_length = valid_length.cpu().numpy() if valid_length.is_cuda else valid_length.numpy()\n            if valid_length.ndim == 1:\n                valid_length = torch.FloatTensor(np.tile(valid_length, self.num_heads))\n            else:\n                valid_length = torch.FloatTensor(np.tile(valid_length, (self.num_heads,1)))\n\n            valid_length = valid_length.to(device)\n            \n        output = self.attention(query, key, value, valid_length)\n        output_concat = transpose_output(output, self.num_heads)\n        return self.W_o(output_concat)\n\ndef transpose_qkv(X, num_heads):\n    # Original X shape: (batch_size, seq_len, hidden_size * num_heads),\n    # -1 means inferring its value, after first reshape, X shape:\n    # (batch_size, seq_len, num_heads, hidden_size)\n    X = X.view(X.shape[0], X.shape[1], num_heads, -1)\n    \n    # After transpose, X shape: (batch_size, num_heads, seq_len, hidden_size)\n    X = X.transpose(2, 1).contiguous()\n\n    # Merge the first two dimensions. Use reverse=True to infer shape from\n    # right to left.\n    # output shape: (batch_size * num_heads, seq_len, hidden_size)\n    output = X.view(-1, X.shape[2], X.shape[3])\n    return output\n\n\n# Saved in the d2l package for later use\ndef transpose_output(X, num_heads):\n    # A reversed version of transpose_qkv\n    X = X.view(-1, num_heads, X.shape[1], X.shape[2])\n    X = X.transpose(2, 1).contiguous()\n    return X.view(X.shape[0], X.shape[1], -1)\n\ncell = MultiHeadAttention(5, 9, 3, 0.5)\nX = torch.ones((2, 4, 5))\nvalid_length = torch.FloatTensor([2, 3])\ncell(X, X, X, valid_length).shape","execution_count":null},{"metadata":{"id":"176A0C34B0FA494C8229AD2A4310308C","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# 基于位置的前馈网络\nTransformer 模块还有一个非常重要的部分——基于位置的前馈网络（FFN），它接受一个形状为（batch_size，seq_length, feature_size）的三维张量。Position-wise FFN由两个全连接层组成，他们作用在最后一维上。因为序列的每个位置的状态都会被单独地更新，所以我们称他为position-wise，这等效于一个1x1的卷积。与多头注意力层相似，FFN层同样只会对最后一维的大小进行改变；除此之外，对于两个完全相同的输入，FFN层的输出也将相等。"},{"metadata":{"id":"7C6DE7A87C694EAC8EB1C9CB6E148E88","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"class PositionWiseFFN(nn.Module):\n    def __init__(self, input_size, ffn_hidden_size, hidden_size_out, **kwargs):\n        super(PositionWiseFFN, self).__init__(**kwargs)\n        self.ffn_1 = nn.Linear(input_size, ffn_hidden_size)\n        self.ffn_2 = nn.Linear(ffn_hidden_size, hidden_size_out)\n        \n        \n    def forward(self, X):\n        return self.ffn_2(F.relu(self.ffn_1(X)))\n\nffn = PositionWiseFFN(4, 4, 8)\nout = ffn(torch.ones((2,3,4)))\n\nprint(out, out.shape)","execution_count":null},{"metadata":{"id":"84E6728CA41947829D7A82E6E0868B4F","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# 相加归一化层\nTransformer还有一个重要的相加归一化层，它可以平滑地整合输入和其他层的输出，因此在每个多头注意力层和FFN层后面都添加一个含残差连接的Layer Norm层。这里 Layer Norm 与Batch Norm很相似，唯一的区别在于Batch Norm是对于batch size这个维度进行计算均值和方差的，而Layer Norm则是对最后一维进行计算。层归一化可以防止层内的数值变化过大，从而有利于加快训练速度并且提高泛化性能。"},{"metadata":{"id":"7B2D22A635794068868458C7FF949E27","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"layernorm = nn.LayerNorm(normalized_shape=2, elementwise_affine=True)\nbatchnorm = nn.BatchNorm1d(num_features=2, affine=True)\nX = torch.FloatTensor([[1,2], [3,4]])\nprint('layer norm:', layernorm(X))\nprint('batch norm:', batchnorm(X))\n\nclass AddNorm(nn.Module):\n    def __init__(self, hidden_size, dropout, **kwargs):\n        super(AddNorm, self).__init__(**kwargs)\n        self.dropout = nn.Dropout(dropout)\n        self.norm = nn.LayerNorm(hidden_size)\n    \n    def forward(self, X, Y):\n        return self.norm(self.dropout(Y) + X)\n\nadd_norm = AddNorm(4, 0.5)\nadd_norm(torch.ones((2,3,4)), torch.ones((2,3,4))).shape","execution_count":null},{"metadata":{"id":"88015BB86BF64E4482494FEF7393A159","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"## 位置编码\nTransformer模型引入位置编码是为了保持输入序列元素的位置。\n![Image Name](https://cdn.kesci.com/upload/image/q5y12fa4zb.PNG?imageView2/0/w/960/h/960)"},{"metadata":{"id":"393E94CBDB18492B8BB598EBA351C2ED","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"class PositionalEncoding(nn.Module):\n    def __init__(self, embedding_size, dropout, max_len=1000):\n        super(PositionalEncoding, self).__init__()\n        self.dropout = nn.Dropout(dropout)\n        self.P = np.zeros((1, max_len, embedding_size))\n        X = np.arange(0, max_len).reshape(-1, 1) / np.power(\n            10000, np.arange(0, embedding_size, 2)/embedding_size)\n        self.P[:, :, 0::2] = np.sin(X)\n        self.P[:, :, 1::2] = np.cos(X)\n        self.P = torch.FloatTensor(self.P)\n    \n    def forward(self, X):\n        if X.is_cuda and not self.P.is_cuda:\n            self.P = self.P.cuda()\n        X = X + self.P[:, :X.shape[1], :]\n        return self.dropout(X)","execution_count":null},{"metadata":{"id":"54B90DABFBF34B11B0ED9D38CC1F240D","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# 编码器\n编码器包含一个多头注意力层，一个position-wise FFN，和两个 Add and Norm层。对于attention模型以及FFN模型，输出维度都是与embedding维度一致的，这也是由于残差连接天生的特性导致的，因为要将前一层的输出与原始输入相加并归一化。整个编码器由n个Encoder Block堆叠而成，因为残差连接的缘故，中间状态的维度始终与嵌入向量的维度d一致；同时注意到我们把嵌入向量乘以根号d以防止其值过小。"},{"metadata":{"id":"3799F38B8E7242B38946BC8050A1FD1B","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"# 编码块\nclass EncoderBlock(nn.Module):\n    def __init__(self, embedding_size, ffn_hidden_size, num_heads,\n                 dropout, **kwargs):\n        super(EncoderBlock, self).__init__(**kwargs)\n        self.attention = MultiHeadAttention(embedding_size, embedding_size, num_heads, dropout)\n        self.addnorm_1 = AddNorm(embedding_size, dropout)\n        self.ffn = PositionWiseFFN(embedding_size, ffn_hidden_size, embedding_size)\n        self.addnorm_2 = AddNorm(embedding_size, dropout)\n\n    def forward(self, X, valid_length):\n        Y = self.addnorm_1(X, self.attention(X, X, X, valid_length))\n        return self.addnorm_2(Y, self.ffn(Y))\n\n# 编码器\nclass TransformerEncoder(d2l.Encoder):\n    def __init__(self, vocab_size, embedding_size, ffn_hidden_size,\n                 num_heads, num_layers, dropout, **kwargs):\n        super(TransformerEncoder, self).__init__(**kwargs)\n        self.embedding_size = embedding_size\n        self.embed = nn.Embedding(vocab_size, embedding_size)\n        self.pos_encoding = PositionalEncoding(embedding_size, dropout)\n        self.blks = nn.ModuleList()\n        for i in range(num_layers):\n            self.blks.append(\n                EncoderBlock(embedding_size, ffn_hidden_size,\n                             num_heads, dropout))\n\n    def forward(self, X, valid_length, *args):\n        X = self.pos_encoding(self.embed(X) * math.sqrt(self.embedding_size))\n        for blk in self.blks:\n            X = blk(X, valid_length)\n        return X\n\n# 测试编码器\nencoder = TransformerEncoder(200, 24, 48, 8, 2, 0.5)\nencoder(torch.ones((2, 100)).long(), valid_length).shape","execution_count":null},{"metadata":{"id":"02EAE67D812C4309A255EBB3AD94296C","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# 解码器\n解码器与编码器结构类似，只是除了上面的几个模块之外，编码器部分有另一个子模块。该模块也是多头注意力层，接受编码器的输出作为key和value，decoder的状态作为query。与编码器部分相类似，解码器同样是使用了add and norm机制，用残差和层归一化将各个子层的输出相连。\n在第t个时间步，当前输入是query，那么self attention接受了第t步以及前t-1步的所有输入。在训练时，由于第t位置的输入可以观测到全部的序列，这与预测阶段的情形项矛盾，所以要通过将第t个时间步所对应的可观测长度设置为t，以消除不需要看到的未来的信息。"},{"metadata":{"id":"B1E16EB5E00747B89190B2BF967A4915","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"# 解码块\nclass DecoderBlock(nn.Module):\n    def __init__(self, embedding_size, ffn_hidden_size, num_heads,dropout,i,**kwargs):\n        super(DecoderBlock, self).__init__(**kwargs)\n        self.i = i\n        self.attention_1 = MultiHeadAttention(embedding_size, embedding_size, num_heads, dropout)\n        self.addnorm_1 = AddNorm(embedding_size, dropout)\n        self.attention_2 = MultiHeadAttention(embedding_size, embedding_size, num_heads, dropout)\n        self.addnorm_2 = AddNorm(embedding_size, dropout)\n        self.ffn = PositionWiseFFN(embedding_size, ffn_hidden_size, embedding_size)\n        self.addnorm_3 = AddNorm(embedding_size, dropout)\n    \n    def forward(self, X, state):\n        enc_outputs, enc_valid_length = state[0], state[1]\n        \n        # state[2][self.i] stores all the previous t-1 query state of layer-i\n        # len(state[2]) = num_layers\n        \n        # If training:\n        #     state[2] is useless.\n        # If predicting:\n        #     In the t-th timestep:\n        #         state[2][self.i].shape = (batch_size, t-1, hidden_size)\n        # Demo:\n        # love dogs ! [EOS]\n        #  |    |   |   |\n        #   Transformer \n        #    Decoder\n        #  |   |   |   |\n        #  I love dogs !\n        \n        if state[2][self.i] is None:\n            key_values = X\n        else:\n            # shape of key_values = (batch_size, t, hidden_size)\n            key_values = torch.cat((state[2][self.i], X), dim=1) \n        state[2][self.i] = key_values\n        \n        if self.training:\n            batch_size, seq_len, _ = X.shape\n            # Shape: (batch_size, seq_len), the values in the j-th column are j+1\n            valid_length = torch.FloatTensor(np.tile(np.arange(1, seq_len+1), (batch_size, 1))) \n            valid_length = valid_length.to(X.device)\n        else:\n            valid_length = None\n\n        X2 = self.attention_1(X, key_values, key_values, valid_length)\n        Y = self.addnorm_1(X, X2)\n        Y2 = self.attention_2(Y, enc_outputs, enc_outputs, enc_valid_length)\n        Z = self.addnorm_2(Y, Y2)\n        return self.addnorm_3(Z, self.ffn(Z)), state\n\n# 解码器\nclass TransformerDecoder(d2l.Decoder):\n    def __init__(self, vocab_size, embedding_size, ffn_hidden_size,\n                 num_heads, num_layers, dropout, **kwargs):\n        super(TransformerDecoder, self).__init__(**kwargs)\n        self.embedding_size = embedding_size\n        self.num_layers = num_layers\n        self.embed = nn.Embedding(vocab_size, embedding_size)\n        self.pos_encoding = PositionalEncoding(embedding_size, dropout)\n        self.blks = nn.ModuleList()\n        for i in range(num_layers):\n            self.blks.append(\n                DecoderBlock(embedding_size, ffn_hidden_size, num_heads,\n                             dropout, i))\n        self.dense = nn.Linear(embedding_size, vocab_size)\n\n    def init_state(self, enc_outputs, enc_valid_length, *args):\n        return [enc_outputs, enc_valid_length, [None]*self.num_layers]\n\n    def forward(self, X, state):\n        X = self.pos_encoding(self.embed(X) * math.sqrt(self.embedding_size))\n        for blk in self.blks:\n            X, state = blk(X, state)\n        return self.dense(X), state","execution_count":null},{"metadata":{"id":"D2699CBDED2140768567CD8FFCAD6665","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"","execution_count":null}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.3","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":0}