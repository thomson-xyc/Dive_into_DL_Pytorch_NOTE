{"cells":[{"metadata":{"cell_type":"code","id":"3AF73A8B24FD4DBDA177A6508AD65CE7","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# 文本预处理\n实现自然语言处理，首先要对待处理文本进行预处理。预处理的一般步骤有：读入文本、分词、建立字典、词序转换索引\n## 读入文本"},{"metadata":{"id":"3798AA16ACB440448AD41CB6001049CF","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"# sentences 3221\n","name":"stdout"}],"source":"import collections\nimport re\n\ndef read_time_machine():\n    with open('/home/kesci/input/timemachine7163/timemachine.txt', 'r') as f:\n        # 按行处理，strip()移除开头结尾的字符，这里是空白；lower()将大写变为小写\n        # re.sub()将用第三个属性的元素替换第二个属性的元素\n        lines = [re.sub('[^a-z]+', ' ', line.strip().lower()) for line in f]\n    return lines\n\nlines = read_time_machine()  # 读入文本行数\nprint('# sentences {}'.format(len(lines)))\n# print(lines)","execution_count":1},{"metadata":{"id":"0941945B5C9F4F879B1A0E68CB8167BE","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"## 分词\n分词就是将句子划分成一堆词，将句子转换成词的序列"},{"metadata":{"id":"0A46CC7211E74DA18E595A15D6BBBB7C","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"execute_result","metadata":{},"data":{"text/plain":"[['the', 'time', 'machine', 'by', 'h', 'g', 'wells', ''], [''], [''], ['']]"},"transient":{},"execution_count":2}],"source":"def tokenize(sentences, token='word'):\n    \"\"\"Split sentences into word or char tokens\"\"\"\n    if token == 'word':  # 在单词级别做分词\n        return [sentence.split(' ') for sentence in sentences]\n    elif token == 'char':\n        return [list(sentence) for sentence in sentences]\n    else:\n        print('ERROR: unkown token type '+token)\n\ntokens = tokenize(lines)\ntokens[0:4]","execution_count":2},{"metadata":{"id":"AA4E38A8675E46F88523DD35E53A0872","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"## 建立字典\n将文本转换成数字，以方便计算机处理；构建一个字典，将每个词映射到一个唯一的索引编号。"},{"metadata":{"id":"2B5E03B8BABD459F87A1FB2ED31F6ADA","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"class Vocab(object):\n    def __init__(self, tokens, min_freq=0, use_special_tokens=False):\n        # tokens存放所有的词,min_freq表示词频阈值，剔除词频很小的词\n        # use_special_tokens表示是否使用特殊字符\n        counter = count_corpus(tokens)  # 记录词出现的次数，返回键值对 \n        self.token_freqs = list(counter.items())  # 将词提取出构成列表\n        self.idx_to_token = []  # 存词的？？\n        if use_special_tokens:\n            # padding, begin of sentence, end of sentence, unknown\n            self.pad, self.bos, self.eos, self.unk = (0, 1, 2, 3)\n            self.idx_to_token += ['pad', 'bos', 'eos', 'unk']\n        else:\n            self.unk = 0\n            self.idx_to_token += ['']\n        self.idx_to_token += [token for token, freq in self.token_freqs\n                        if freq >= min_freq and token not in self.idx_to_token]\n        self.token_to_idx = dict()\n        for idx, token in enumerate(self.idx_to_token):\n            self.token_to_idx[token] = idx\n\n    def __len__(self):\n        return len(self.idx_to_token)\n\n    def __getitem__(self, tokens):  # 类索引；词到索引的映射，给定词，返回索引\n        if not isinstance(tokens, (list, tuple)):\n            return self.token_to_idx.get(tokens, self.unk)\n        return [self.__getitem__(token) for token in tokens]\n\n    def to_tokens(self, indices):  # 索引到词的映射，给定索引，返回词\n        if not isinstance(indices, (list, tuple)):\n            return self.idx_to_token[indices]\n        return [self.idx_to_token[index] for index in indices]\n\ndef count_corpus(sentences):\n    tokens = [tk for st in sentences for tk in st]\n    return collections.Counter(tokens)  # 返回一个字典，记录每个词的出现次数\n","execution_count":3},{"metadata":{"id":"1F2E7ECA64CF44B8927D04D04DB0C38F","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"[('', 0), ('the', 1), ('time', 2), ('machine', 3), ('by', 4), ('h', 5), ('g', 6), ('wells', 7), ('i', 8), ('traveller', 9)]\n","name":"stdout"}],"source":"# 用Time Machine作为语料构建字典\nvocab = Vocab(tokens)\nprint(list(vocab.token_to_idx.items())[0:10])","execution_count":4},{"metadata":{"id":"7DF7B591BECC46A28ED48FFC5356ACB1","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"## 将词转换为索引\n利用字典将原文本中的句子从单词序列转换为索引序列"},{"metadata":{"id":"FFCCEEBB9DA64CDEB5A0670F3A47255A","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"words: ['the', 'time', 'traveller', 'for', 'so', 'it', 'will', 'be', 'convenient', 'to', 'speak', 'of', 'him', '']\nindices: [1, 2, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 0]\nwords: ['was', 'expounding', 'a', 'recondite', 'matter', 'to', 'us', 'his', 'grey', 'eyes', 'shone', 'and']\nindices: [20, 21, 22, 23, 24, 16, 25, 26, 27, 28, 29, 30]\n","name":"stdout"}],"source":"for i in range(8, 10):\n    print('words:', tokens[i])\n    print('indices:', vocab[tokens[i]])","execution_count":5},{"metadata":{"id":"E06D961B392B417DBA4E760FD7FF3C60","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"## 用现有工具进行分词\n前面的分词方式至少有以下几个缺点:\n1.标点符号通常可以提供语义信息，但是我们的方法直接将其丢弃了\n2.类似“shouldn't\", \"doesn't\"这样的词会被错误地处理\n3.类似\"Mr.\", \"Dr.\"这样的词会被错误地处理\n用两个工具spaCy和NLTK进行改善"},{"metadata":{"id":"7D64093255B3444491E55D4440A4DE55","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"No module named 'spacy'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-caab89c1898d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Mr. Chen doesn't agree with my suggestion.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# spacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'en_core_web_sm'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"]}],"source":"text = \"Mr. Chen doesn't agree with my suggestion.\"\n# spacy\nimport spacy\nnlp = spacy.load('en_core_web_sm')\ndoc = nlp(text)\nprint([token.text for token in doc])\n# NLTK\nfrom nltk.tokenize import word_tokenize\nfrom nltk import data\ndata.path.append('/home/kesci/input/nltk_data3784/nltk_data')\nprint(word_tokenize(text))","execution_count":6},{"metadata":{"id":"56AED192FB364D4C8F27CB1BC0584128","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# 语言模型\n   自然语言处理要对文本进行处理，实现语音识别或机器翻译等文本的成型或转换，就不止需要把相应的读音转换成文本或将单个词翻译为另一语言的词，还需要考虑句子整体的连贯性，即要针对上下文来理解句子语义，以避免同音词的误读或一词多义导致的错译。对于我们人而言，我们需要学习语法，那么对于机器，我觉得，语言模型就可以看做是用来学习语法的工具。\n我是这样理解的：给定一段长度为T的词的序列，语言模型将计算该序列的概率：P(w1, w2, …, wT)。假设我们要实现英译汉，给出一段英文文本序列，我们根据单词释义产生若干可能的汉语翻译文本，通过对每段翻译文本序列求概率并进行比较，就可以选出最优的翻译文本（概率最大的序列）。\n   如何计算语言模型呢？因为序列中每个词是依次生成的，那么根据概率乘法公式，可得到：\n![Image Name](https://cdn.kesci.com/upload/image/q5oqpynul4.PNG?imageView2/0/w/960/h/960)例如，一段含4个词的文本序列的概率为：![Image Name](https://cdn.kesci.com/upload/image/q5oqqf5m53.PNG?imageView2/0/w/960/h/960)\n   那么，公式中的每个词的概率如何计算？\n   我们假设用若干篇小说作为训练集来训练模型。这时，我们要求一段生成文本的概率，就要通过对训练集中这些词出现的次数进行统计。P(w1)就可以根据n(w1)/n来计算，n(A)代表A词在训练集出现的次数，n代表整个训练集的单词个数。P(w2|w1)就可以根据n(w1,w2)/n(w1)来计算，其中n(w1,w2)就是w1和w2作为整个单词出现的次数。这样，我们就可以计算出整个句子序列的概率，这就有点类似于我们人类经过学习可以通过语法语义对句子进行处理，而机器通过统计词频 “学习”根据语法语义处理句子。\n## n元语法\n   我们可以发现上面的概率计算理论上没问题，但实际中会随着句子长度的增加导致概率愈发地复杂。这时候，我们就要用到n元语法。n元语法是通过马尔可夫假设（不一定成立）简化语言模型的计算，即一个词的出现只与前面出现的n个词相关（n阶马尔可夫链）。基于n-1阶马尔可夫链，语言模型可表示成：\n![Image Name](https://cdn.kesci.com/upload/image/q5oqs0ener.PNG?imageView2/0/w/960/h/960)\n这个也叫n元语法(n-grams)。再用上面4个词的文本序列作为例子，3元语法的概率可表示为：\n![Image Name](https://cdn.kesci.com/upload/image/q5oqsl6442.PNG?imageView2/0/w/960/h/960)"},{"metadata":{"id":"A7C231143FE943A4B2EB65ABF78D25D4","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"63282\n想要有直升机\n想要和你飞到宇宙去\n想要和你融化在一起\n融化在宇宙里\n我每天每天每\n1027\nchars: 想要有直升机 想要和你飞到宇宙去 想要和\nindices: [700, 718, 638, 805, 845, 104, 371, 700, 718, 733, 797, 551, 688, 632, 616, 533, 371, 700, 718, 733]\n","name":"stdout"}],"source":"# 读取数据集\nwith open('/home/kesci/input/jaychou_lyrics4703/jaychou_lyrics.txt') as f:\n    corpus_chars = f.read()\nprint(len(corpus_chars))\nprint(corpus_chars[: 40])\ncorpus_chars = corpus_chars.replace('\\n', ' ').replace('\\r', ' ')\ncorpus_chars = corpus_chars[: 10000]\n\n# 建立字符索引\nidx_to_char = list(set(corpus_chars)) # 去重，得到索引到字符的映射，可用下表取出字符\nchar_to_idx = {char: i for i, char in enumerate(idx_to_char)} # 字符到索引的映射， 构建一个字典\nvocab_size = len(char_to_idx)\nprint(vocab_size)\n\ncorpus_indices = [char_to_idx[char] for char in corpus_chars]  # 将每个字符转化为索引，得到一个索引的序列\nsample = corpus_indices[: 20]\nprint('chars:', ''.join([idx_to_char[idx] for idx in sample]))\nprint('indices:', sample)\n\n# 定义函数，后续使用\ndef load_data_jay_lyrics():\n    with open('/home/kesci/input/jaychou_lyrics4703/jaychou_lyrics.txt') as f:\n        corpus_chars = f.read()\n    corpus_chars = corpus_chars.replace('\\n', ' ').replace('\\r', ' ')\n    corpus_chars = corpus_chars[0:10000]\n    idx_to_char = list(set(corpus_chars))\n    char_to_idx = dict([(char, i) for i, char in enumerate(idx_to_char)])\n    vocab_size = len(char_to_idx)\n    corpus_indices = [char_to_idx[char] for char in corpus_chars]\n    return corpus_indices, char_to_idx, idx_to_char, vocab_size","execution_count":7},{"metadata":{"id":"F54A2FFE7662414E98413CA9993854FA","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"## 语言模型数据集\n### 数据的采样\n如果序列的长度为T，采样步数为n，那么一共有T-n个合法的样本，但是这些样本有大量的重合，我们通常采用更加高效的采样方式。我们有两种方式对数据进行采样，分别是随机采样和相邻采样。\n### 随机采样\n在随机采样中，每个样本是原始序列上任意截取的一段序列，相邻的两个随机小批量在原始序列上的位置不一定相毗邻。\n![Image Name](https://cdn.kesci.com/upload/image/q5oqyr1656.PNG?imageView2/0/w/960/h/960)\n如上图，随机采样根据时间步长将样本划分为若干等长的子序列，然后在每个子序列中选取批量值个数的数据进行批量处理。"},{"metadata":{"id":"34F7147529A94EDD8C21CD54191B6F2D","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"X:  tensor([[ 0,  1,  2,  3,  4,  5],\n        [ 6,  7,  8,  9, 10, 11]]) \nY: tensor([[ 1,  2,  3,  4,  5,  6],\n        [ 7,  8,  9, 10, 11, 12]]) \n\nX:  tensor([[18, 19, 20, 21, 22, 23],\n        [12, 13, 14, 15, 16, 17]]) \nY: tensor([[19, 20, 21, 22, 23, 24],\n        [13, 14, 15, 16, 17, 18]]) \n\n","name":"stdout"}],"source":"# 随机采样\nimport torch\nimport random\ndef data_iter_random(corpus_indices, batch_size, num_steps, device=None):\n    # 减1是因为对于长度为n的序列，X最多只有包含其中的前n - 1个字符\n    num_examples = (len(corpus_indices) - 1) // num_steps  # 下取整，得到不重叠情况下的样本个数\n    example_indices = [i * num_steps for i in range(num_examples)]  # 每个样本的第一个字符在corpus_indices中的下标\n    random.shuffle(example_indices)\n\n    def _data(i):\n        # 返回从i开始的长为num_steps的序列\n        return corpus_indices[i: i + num_steps]\n    if device is None:\n        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    \n    for i in range(0, num_examples, batch_size):\n        # 每次选出batch_size个随机样本\n        batch_indices = example_indices[i: i + batch_size]  # 当前batch的各个样本的首字符的下标\n        X = [_data(j) for j in batch_indices]\n        Y = [_data(j + 1) for j in batch_indices]\n        yield torch.tensor(X, device=device), torch.tensor(Y, device=device)\n\nmy_seq = list(range(30))\nfor X, Y in data_iter_random(my_seq, batch_size=2, num_steps=6):\n    print('X: ', X, '\\nY:', Y, '\\n')","execution_count":8},{"metadata":{"id":"33683E696287474E8AE8CC49E98CD8F7","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"### 相邻采样\n在相邻采样中，相邻的两个随机小批量在原始序列上的位置相毗邻。\n![Image Name](https://cdn.kesci.com/upload/image/q5or3vrnjz.PNG?imageView2/0/w/960/h/960)\n如图，相邻采样将样本划分为等步长的子序列，然后在每个子序列中选取等批量值的数据组成一个批量。"},{"metadata":{"id":"A3BF2E7324274CDF9AA4C2B0128300B7","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"X:  tensor([[ 0,  1,  2,  3,  4,  5],\n        [15, 16, 17, 18, 19, 20]]) \nY: tensor([[ 1,  2,  3,  4,  5,  6],\n        [16, 17, 18, 19, 20, 21]]) \n\nX:  tensor([[ 6,  7,  8,  9, 10, 11],\n        [21, 22, 23, 24, 25, 26]]) \nY: tensor([[ 7,  8,  9, 10, 11, 12],\n        [22, 23, 24, 25, 26, 27]]) \n\n","name":"stdout"}],"source":"# 相邻采样\ndef data_iter_consecutive(corpus_indices, batch_size, num_steps, device=None):\n    if device is None:\n        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    corpus_len = len(corpus_indices) // batch_size * batch_size  # 保留下来的序列的长度\n    corpus_indices = corpus_indices[: corpus_len]  # 仅保留前corpus_len个字符\n    indices = torch.tensor(corpus_indices, device=device)\n    indices = indices.view(batch_size, -1)  # resize成(batch_size, )\n    batch_num = (indices.shape[1] - 1) // num_steps\n    for i in range(batch_num):\n        i = i * num_steps\n        X = indices[:, i: i + num_steps]\n        Y = indices[:, i + 1: i + num_steps + 1]\n        yield X, Y\n        \nfor X, Y in data_iter_consecutive(my_seq, batch_size=2, num_steps=6):\n    print('X: ', X, '\\nY:', Y, '\\n')","execution_count":9}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.3","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":0}