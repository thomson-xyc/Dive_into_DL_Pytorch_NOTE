{"cells":[{"metadata":{"cell_type":"code","id":"41C1BC8F585E49EABAA09CD9C61A1369","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# 过拟合、欠拟合及其解决方案\n## 训练误差&泛化误差\n训练误差：模型训练过程中预测结果的误差。\n泛化误差：模型在实际运用中预测结果的误差，泛化误差的大小直接关系到模型的好坏，要训练好的模型就要使泛化误差尽可能的小。\n训练集：用于训练模型的数据集。\n验证集：用于验证模型好坏的数据集。\nK折交叉验证：把原始训练数据集划分成K个不重合的子数据集，然后对模型做K次训练和验证，每次用一个子数据集验证模型，其余数据集训练模型，最后对K次的训练误差和泛化误差分别求平均。\n# 权重衰减\n权重衰减等价于L2范数正则化。正则化是通过给损失函数添加惩罚项来减小模型参数，以应对过拟合。\n## L2范数正则化\n正则化是给损失函数添加惩罚项，所以L2范数正则化就是给损失函数添加L2范数惩罚项，L2范数惩罚项是各权重参数的平方和与一个正常数的乘积，以线性回归为例，假设线性回归损失函数为：\n![Image Name](https://cdn.kesci.com/upload/image/q5qtqkhicj.PNG?imageView2/0/w/960/h/960)\n则L2范数正则化后损失函数为：\n![Image Name](https://cdn.kesci.com/upload/image/q5qtr1bonl.PNG?imageView2/0/w/960/h/960)\n其中λ是大于0的 超参数。可以看出，λ较大时，惩罚项在损失函数中比重较大，会导致权重参数更接近0；λ较小时，惩罚项的作用相对较小。对于绝对值较大的模型参数，权重衰减通过惩罚它们为模型增加了限制，一般对过拟合有效。\n## pytorch简洁实现"},{"metadata":{"id":"FCE6A95ED28F438AA94CE40106CB8C41","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"1.3.0\n","name":"stdout"}],"source":"%matplotlib inline\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport sys\nsys.path.append(\"/home/kesci/input\")\nimport d2lzh1981 as d2l\n\nprint(torch.__version__)\n\n# 初始化模型参数\nn_train, n_test, num_inputs = 20, 100, 200\ntrue_w, true_b = torch.ones(num_inputs, 1) * 0.01, 0.05\n\nfeatures = torch.randn((n_train + n_test, num_inputs))\nlabels = torch.matmul(features, true_w) + true_b\nlabels += torch.tensor(np.random.normal(0, 0.01, size=labels.size()), dtype=torch.float)\ntrain_features, test_features = features[:n_train, :], features[n_train:, :]\ntrain_labels, test_labels = labels[:n_train], labels[n_train:]\n\n# 定义训练和测试\nbatch_size, num_epochs, lr = 1, 100, 0.003\nnet, loss = d2l.linreg, d2l.squared_loss\n\ndataset = torch.utils.data.TensorDataset(train_features, train_labels)\ntrain_iter = torch.utils.data.DataLoader(dataset, batch_size, shuffle=True)","execution_count":1},{"metadata":{"id":"4189B2B9CC904E8A8A13D11342F9DD39","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"def fit_and_plot_pytorch(wd):\n    # 对权重参数衰减。权重名称一般是以weight结尾\n    net = nn.Linear(num_inputs, 1)\n    nn.init.normal_(net.weight, mean=0, std=1)\n    nn.init.normal_(net.bias, mean=0, std=1)\n    optimizer_w = torch.optim.SGD(params=[net.weight], lr=lr, weight_decay=wd) # 对权重参数衰减\n    optimizer_b = torch.optim.SGD(params=[net.bias], lr=lr)  # 不对偏差参数衰减\n    \n    train_ls, test_ls = [], []\n    for _ in range(num_epochs):\n        for X, y in train_iter:\n            l = loss(net(X), y).mean()\n            optimizer_w.zero_grad()\n            optimizer_b.zero_grad()\n            \n            l.backward()\n            \n            # 对两个optimizer实例分别调用step函数，从而分别更新权重和偏差\n            optimizer_w.step()\n            optimizer_b.step()\n        train_ls.append(loss(net(train_features), train_labels).mean().item())\n        test_ls.append(loss(net(test_features), test_labels).mean().item())\n    d2l.semilogy(range(1, num_epochs + 1), train_ls, 'epochs', 'loss',\n                 range(1, num_epochs + 1), test_ls, ['train', 'test'])\n    print('L2 norm of w:', net.weight.data.norm().item())","execution_count":2},{"metadata":{"id":"6CB675640FB34A0BAC1CA4EACD07817C","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"L2 norm of w: 12.73517894744873\n","name":"stdout"},{"output_type":"display_data","metadata":{"needs_background":"light"},"data":{"text/plain":"<Figure size 252x180 with 1 Axes>","text/html":"<img src=\"https://cdn.kesci.com/rt_upload/6CB675640FB34A0BAC1CA4EACD07817C/q5r5vi1jyj.svg\">"},"transient":{}}],"source":"fit_and_plot_pytorch(0)","execution_count":3},{"metadata":{"id":"A11268E71DDE4C7F8A5E2B454D26D023","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"L2 norm of w: 0.038453906774520874\n","name":"stdout"},{"output_type":"display_data","metadata":{"needs_background":"light"},"data":{"text/plain":"<Figure size 252x180 with 1 Axes>","text/html":"<img src=\"https://cdn.kesci.com/rt_upload/A11268E71DDE4C7F8A5E2B454D26D023/q5r5vmgt3w.svg\">"},"transient":{}}],"source":"fit_and_plot_pytorch(3)","execution_count":4},{"metadata":{"id":"75FDFA705B4D4A70AB2951950A9D6417","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# 倒置丢弃法\n丢弃法是深度学习模型常用的应对过拟合的方法。这里笔记倒置丢弃法。\n先看一个多层感知机：\n![Image Name](https://cdn.kesci.com/upload/image/q5qyh2yq89.png?imageView2/0/w/960/h/960)\n这个感知机有一个隐藏层，对这个隐藏层使用丢弃法就是随机将该层中几个神经元丢弃。使用了丢弃法后此感知机可能变成这个样子：\n![Image Name](https://cdn.kesci.com/upload/image/q5qyhksdfc.png?imageView2/0/w/960/h/960)\n假设丢弃概率为p，那么hi被丢弃的概率就是p，而hi剩下1-p的概率会除以1-p进行拉伸。假设随机变量ri为0或1，为0的概率为p，为1的概率为1-p，则用丢弃法计算新的隐藏单元hi'为：\n![Image Name](https://cdn.kesci.com/upload/image/q5qyi9io8q.PNG?imageView2/0/w/960/h/960)\n这里除以1-p是为了使新隐藏单元的期望不变：\n![Image Name](https://cdn.kesci.com/upload/image/q5qyihisnq.PNG?imageView2/0/w/960/h/960)\n## pytorch简洁实现"},{"metadata":{"id":"4B582BC1944F4EFE8111820009BD0B89","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"1.3.0\n","name":"stdout"}],"source":"# 准备工作\n%matplotlib inline\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport sys\nsys.path.append(\"/home/kesci/input\")\nimport d2lzh1981 as d2l\n\nprint(torch.__version__)\n\n# 参数的初始化\nnum_inputs, num_outputs, num_hiddens1, num_hiddens2 = 784, 10, 256, 256\n\nW1 = torch.tensor(np.random.normal(0, 0.01, size=(num_inputs, num_hiddens1)), dtype=torch.float, requires_grad=True)\nb1 = torch.zeros(num_hiddens1, requires_grad=True)\nW2 = torch.tensor(np.random.normal(0, 0.01, size=(num_hiddens1, num_hiddens2)), dtype=torch.float, requires_grad=True)\nb2 = torch.zeros(num_hiddens2, requires_grad=True)\nW3 = torch.tensor(np.random.normal(0, 0.01, size=(num_hiddens2, num_outputs)), dtype=torch.float, requires_grad=True)\nb3 = torch.zeros(num_outputs, requires_grad=True)\n\nparams = [W1, b1, W2, b2, W3, b3]\n\ndrop_prob1, drop_prob2 = 0.2, 0.5\n\nnum_epochs, lr, batch_size = 5, 100.0, 256  # 这里的学习率设置的很大，原因与之前相同。\nloss = torch.nn.CrossEntropyLoss()\ntrain_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, root='/home/kesci/input/FashionMNIST2065')","execution_count":5},{"metadata":{"id":"9CF6FE5DDEF6422882EB4F1EE38CC4E8","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"# 网络搭建\nnet = nn.Sequential(\n        d2l.FlattenLayer(),\n        nn.Linear(num_inputs, num_hiddens1),\n        nn.ReLU(),\n        nn.Dropout(drop_prob1),\n        nn.Linear(num_hiddens1, num_hiddens2), \n        nn.ReLU(),\n        nn.Dropout(drop_prob2),\n        nn.Linear(num_hiddens2, 10)\n        )\n\nfor param in net.parameters():\n    nn.init.normal_(param, mean=0, std=0.01)","execution_count":6},{"metadata":{"id":"E9CA8FAD7DB149E9AB6B1BBFB2135B01","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"epoch 1, loss 0.0044, train acc 0.559, test acc 0.703\nepoch 2, loss 0.0023, train acc 0.781, test acc 0.743\nepoch 3, loss 0.0019, train acc 0.820, test acc 0.805\nepoch 4, loss 0.0018, train acc 0.836, test acc 0.819\nepoch 5, loss 0.0016, train acc 0.848, test acc 0.774\n","name":"stdout"}],"source":"optimizer = torch.optim.SGD(net.parameters(), lr=0.5)\nd2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, None, None, optimizer)","execution_count":7},{"metadata":{"id":"E032E8E05C3A433ABBE98D0F86181A23","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"","execution_count":null}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.3","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":0}