{"cells":[{"metadata":{"cell_type":"code","id":"AF81294BF570443B858375C4D0F6316E","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"# 生成对抗网络\n之前学习的深度学习算法都是判别式的学习算法，现在，我们期望通过对大量数据如图片数据进行学习，然后通过学习自动产生一个新的逼真的图片，这种方式叫作生成式学习。生成对抗网络（GANs）是典型的生成式模型。\n生成对抗网络由2个重要的部分构成：\n生成器(Generator)：通过机器生成数据（大部分情况下是图像），目的是“骗过”判别器\n判别器(Discriminator)：判断这张图像是真实的还是机器生成的，目的是找出生成器做的“假数据”\n生成对抗网络的实现如下：\n首先，生成器和判别器都是一个神经网络。在最初的论文中，使用的是MLP。生成器的输入是噪声，噪声输入到生成器中，输出是一个图片。此时的生成器能力还很差，生成的图片质量很差，然后将真实的图片与生成的图片一起输入到判别器中，让判别器去判别那个是真那个是假。当判别器v1能够很好地判别出图像的真假时，固定判别器v1的参数。继续训练生成器，不断的调整生成器的参数，直到判别器不能判别出生成的图像是假图像。此时的生成器v2生成图片要比之前生成器v1好一些。然后继续训练判别器，一样达到判别器很好的判别生成器v2的图片是假图片。以此类推，不断的进行下去。最后。达到的转态是判别器无法识别出图片的真假。此时生成器生成的图片和真实图片基本类似。\n### 代码实现"},{"metadata":{"id":"94B1A10399754FAABBA648F6A17B11BE","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"%matplotlib inline\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import DataLoader\nfrom torch import nn\nimport numpy as np\nfrom torch.autograd import Variable\nimport torch","execution_count":null},{"metadata":{"id":"7D6318929C96471891187C1EB2D3D13F","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"# 生成“真实”数据\nX=np.random.normal(size=(1000,2))\nA=np.array([[1,2],[-0.1,0.5]])\nb=np.array([1,2])\ndata=X.dot(A)+b\n\nplt.figure(figsize=(3.5,2.5))\nplt.scatter(X[:100,0],X[:100,1],color='red')\nplt.show()\nplt.figure(figsize=(3.5,2.5))\nplt.scatter(data[:100,0],data[:100,1],color='blue')\nplt.show()\nprint(\"The covariance matrix is\\n%s\" % np.dot(A.T, A))\n\nbatch_size=8\ndata_iter=DataLoader(data,batch_size=batch_size)","execution_count":null},{"metadata":{"id":"21D76A1D9B334DC49B4609000636C7FB","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"# 生成器\nclass net_G(nn.Module):\n    def __init__(self):\n        super(net_G,self).__init__()\n        self.model=nn.Sequential(\n            nn.Linear(2,2),\n        )\n        self._initialize_weights()\n    def forward(self,x):\n        x=self.model(x)\n        return x\n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m,nn.Linear):\n                m.weight.data.normal_(0,0.02)\n                m.bias.data.zero_()\n\n# 判别器\nclass net_D(nn.Module):\n    def __init__(self):\n        super(net_D,self).__init__()\n        self.model=nn.Sequential(\n            nn.Linear(2,5),\n            nn.Tanh(),\n            nn.Linear(5,3),\n            nn.Tanh(),\n            nn.Linear(3,1),\n            nn.Sigmoid()\n        )\n        self._initialize_weights()\n    def forward(self,x):\n        x=self.model(x)\n        return x\n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m,nn.Linear):\n                m.weight.data.normal_(0,0.02)\n                m.bias.data.zero_()","execution_count":null},{"metadata":{"id":"A5887F1129224D228CFCCC900FEAB0A1","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"# 训练\n# 生成器要最大化交叉熵损失，判别器要最小化交叉熵损失\n# 更新判别器\ndef update_D(X,Z,net_D,net_G,loss,trainer_D):\n    batch_size=X.shape[0]\n    Tensor=torch.FloatTensor\n    ones=Variable(Tensor(np.ones(batch_size))).view(batch_size,1)\n    zeros = Variable(Tensor(np.zeros(batch_size))).view(batch_size,1)\n    real_Y=net_D(X.float())\n    fake_X=net_G(Z)\n    fake_Y=net_D(fake_X)\n    loss_D=(loss(real_Y,ones)+loss(fake_Y,zeros))/2\n    loss_D.backward()\n    trainer_D.step()\n    return float(loss_D.sum())\n\n# 更新生成器（使用交叉熵损失）\ndef update_G(Z,net_D,net_G,loss,trainer_G):\n    batch_size=Z.shape[0]\n    Tensor=torch.FloatTensor\n    ones=Variable(Tensor(np.ones((batch_size,)))).view(batch_size,1)\n    fake_X=net_G(Z)\n    fake_Y=net_D(fake_X)\n    loss_G=loss(fake_Y,ones)\n    loss_G.backward()\n    trainer_G.step()\n    return float(loss_G.sum())\n\ndef train(net_D,net_G,data_iter,num_epochs,lr_D,lr_G,latent_dim,data):\n    loss=nn.BCELoss()\n    Tensor=torch.FloatTensor\n    trainer_D=torch.optim.Adam(net_D.parameters(),lr=lr_D)\n    trainer_G=torch.optim.Adam(net_G.parameters(),lr=lr_G)\n    plt.figure(figsize=(7,4))\n    d_loss_point=[]\n    g_loss_point=[]\n    d_loss=0\n    g_loss=0\n    for epoch in range(1,num_epochs+1):\n        d_loss_sum=0\n        g_loss_sum=0\n        batch=0\n        for X in data_iter:\n            batch+=1\n            X=Variable(X)\n            batch_size=X.shape[0]\n            Z=Variable(Tensor(np.random.normal(0,1,(batch_size,latent_dim))))\n            trainer_D.zero_grad()\n            d_loss = update_D(X, Z, net_D, net_G, loss, trainer_D)\n            d_loss_sum+=d_loss\n            trainer_G.zero_grad()\n            g_loss = update_G(Z, net_D, net_G, loss, trainer_G)\n            g_loss_sum+=g_loss\n        d_loss_point.append(d_loss_sum/batch)\n        g_loss_point.append(g_loss_sum/batch)\n    plt.ylabel('Loss', fontdict={'size': 14})\n    plt.xlabel('epoch', fontdict={'size': 14})\n    plt.xticks(range(0,num_epochs+1,3))\n    plt.plot(range(1,num_epochs+1),d_loss_point,color='orange',label='discriminator')\n    plt.plot(range(1,num_epochs+1),g_loss_point,color='blue',label='generator')\n    plt.legend()\n    plt.show()\n    print(d_loss,g_loss)\n    \n    Z =Variable(Tensor( np.random.normal(0, 1, size=(100, latent_dim))))\n    fake_X=net_G(Z).detach().numpy()\n    plt.figure(figsize=(3.5,2.5))\n    plt.scatter(data[:,0],data[:,1],color='blue',label='real')\n    plt.scatter(fake_X[:,0],fake_X[:,1],color='orange',label='generated')\n    plt.legend()\n    plt.show()","execution_count":null},{"metadata":{"id":"6E8445B629A64393889C346A9AA185A0","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"# 指定超参数以适应高斯分布\nif __name__ == '__main__':\n    lr_D,lr_G,latent_dim,num_epochs=0.05,0.005,2,20\n    generator=net_G()\n    discriminator=net_D()\n    train(discriminator,generator,data_iter,num_epochs,lr_D,lr_G,latent_dim,data)","execution_count":null},{"metadata":{"id":"43BC636D6E904A2590750643E2984219","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"## GANs的缺点\n训练GAN需要达到纳什均衡,有时候可以用梯度下降法做到,有时候做不到；\nGAN不适合处理离散形式的数据，比如文本；\nGAN存在训练不稳定、梯度消失、模式崩溃的问题。\n# 多层卷积生成对抗网络（DCGAN）\nDCGAN进行了如下的改进：\n\t使用步长卷积代替上采样层，卷积在提取图像特征上具有很好的作用，并且使用卷积代替全连接层；\n\t生成器G和判别器D中几乎每一层都使用batchnorm层，将特征层的输出归一化到一起，加速了训练，提升了训练的稳定性。（生成器的最后一层和判别器的第一层不加batchnorm）；\n\t在判别器中使用leakrelu激活函数，而不是RELU，防止梯度稀疏，生成器中仍然采用relu，但是输出层采用tanh；\n\t使用adam优化器训练，并且学习率最好是0.0002。（有的实验者也试过其他学习率，但不得不说0.0002是表现最好的了）\n\t生成器G和判别器D中几乎每一层都使用batchnorm层，将特征层的输出归一化到一起，加速了训练，提升了训练的稳定性。（生成器的最后一层和判别器的第一层不加batchnorm）。\n### 记注\nDCGAN架构有四个卷积层用于鉴别器，四个“fractionally-strided”卷积层用于生成器。\n判别器包括具有批处理规范化(除了它的输入层)和Leaky ReLU激活的4层带状卷积。\nLeaky ReLU是一个非线性函数，它为负输入提供非零输出。它的目的是修复“死亡ReLU”问题，并帮助梯度流更容易通过架构。\n### 代码实现"},{"metadata":{"id":"D3068D94B3DE4758B8D808BF886EEED2","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"import matplotlib.pyplot as plt\nfrom torch.utils.data import DataLoader\nfrom torch import nn\nimport numpy as np\nfrom torch.autograd import Variable\nimport torch\nfrom torchvision.datasets import ImageFolder\nfrom torchvision.transforms import transforms\nimport zipfile\ncuda = True if torch.cuda.is_available() else False\nprint(cuda)","execution_count":null},{"metadata":{"id":"2A9BB8EE3071418EB60BCBB434F69B8A","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"# 载入数据\ndata_dir='/home/kesci/input/pokemon8600/'\nbatch_size=256\ntransform=transforms.Compose([\n    transforms.Resize((64,64)),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n])\npokemon=ImageFolder(data_dir+'pokemon',transform)\ndata_iter=DataLoader(pokemon,batch_size=batch_size,shuffle=True)\n\nfig=plt.figure(figsize=(4,4))\nimgs=data_iter.dataset.imgs\nfor i in range(20):\n    img = plt.imread(imgs[i*150][0])\n    plt.subplot(4,5,i+1)\n    plt.imshow(img)\n    plt.axis('off')\nplt.show()","execution_count":null},{"metadata":{"id":"0A277AB5EBA647E48C53BC0B872B4D1C","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"# 生成器\nclass G_block(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size=4,strides=2, padding=1):\n        super(G_block,self).__init__()\n        self.conv2d_trans=nn.ConvTranspose2d(in_channels, out_channels, kernel_size=kernel_size,\n                                             stride=strides, padding=padding, bias=False)\n        self.batch_norm=nn.BatchNorm2d(out_channels,0.8)\n        self.activation=nn.ReLU()\n    def forward(self,x):\n        return self.activation(self.batch_norm(self.conv2d_trans(x)))\n\nclass net_G(nn.Module):\n    def __init__(self,in_channels):\n        super(net_G,self).__init__()\n\n        n_G=64\n        self.model=nn.Sequential(\n            G_block(in_channels,n_G*8,strides=1,padding=0),\n            G_block(n_G*8,n_G*4),\n            G_block(n_G*4,n_G*2),\n            G_block(n_G*2,n_G),\n            nn.ConvTranspose2d(\n                n_G,3,kernel_size=4,stride=2,padding=1,bias=False\n            ),\n            nn.Tanh()\n        )\n    def forward(self,x):\n        x=self.model(x)\n        return x\n\n\ndef weights_init_normal(m):\n    classname = m.__class__.__name__\n    if classname.find(\"Conv\") != -1:\n        torch.nn.init.normal_(m.weight.data, mean=0, std=0.02)\n    elif classname.find(\"BatchNorm2d\") != -1:\n        torch.nn.init.normal_(m.weight.data, mean=1.0, std=0.02)\n        torch.nn.init.constant_(m.bias.data, 0.0)\n\n# 判别器\nclass D_block(nn.Module):\n    def __init__(self,in_channels,out_channels,kernel_size=4,strides=2,\n                 padding=1,alpha=0.2):\n        super(D_block,self).__init__()\n        self.conv2d=nn.Conv2d(in_channels,out_channels,kernel_size,strides,padding,bias=False)\n        self.batch_norm=nn.BatchNorm2d(out_channels,0.8)\n        self.activation=nn.LeakyReLU(alpha)\n    def forward(self,X):\n        return self.activation(self.batch_norm(self.conv2d(X)))\n\nclass net_D(nn.Module):\n    def __init__(self,in_channels):\n        super(net_D,self).__init__()\n        n_D=64\n        self.model=nn.Sequential(\n            D_block(in_channels,n_D),\n            D_block(n_D,n_D*2),\n            D_block(n_D*2,n_D*4),\n            D_block(n_D*4,n_D*8)\n        )\n        self.conv=nn.Conv2d(n_D*8,1,kernel_size=4,bias=False)\n        self.activation=nn.Sigmoid()\n        # self._initialize_weights()\n    def forward(self,x):\n        x=self.model(x)\n        x=self.conv(x)\n        x=self.activation(x)\n        return x","execution_count":null},{"metadata":{"id":"FD43B53B1A4740099EBEBF8BC4B23612","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"# 训练\ndef update_D(X,Z,net_D,net_G,loss,trainer_D):\n    batch_size=X.shape[0]\n    Tensor=torch.cuda.FloatTensor\n    ones=Variable(Tensor(np.ones(batch_size,)),requires_grad=False).view(batch_size,1)\n    zeros = Variable(Tensor(np.zeros(batch_size,)),requires_grad=False).view(batch_size,1)\n    real_Y=net_D(X).view(batch_size,-1)\n    fake_X=net_G(Z)\n    fake_Y=net_D(fake_X).view(batch_size,-1)\n    loss_D=(loss(real_Y,ones)+loss(fake_Y,zeros))/2\n    loss_D.backward()\n    trainer_D.step()\n    return float(loss_D.sum())\n\ndef update_G(Z,net_D,net_G,loss,trainer_G):\n    batch_size=Z.shape[0]\n    Tensor=torch.cuda.FloatTensor\n    ones=Variable(Tensor(np.ones((batch_size,))),requires_grad=False).view(batch_size,1)\n    fake_X=net_G(Z)\n    fake_Y=net_D(fake_X).view(batch_size,-1)\n    loss_G=loss(fake_Y,ones)\n    loss_G.backward()\n    trainer_G.step()\n    return float(loss_G.sum())\n\n\ndef train(net_D,net_G,data_iter,num_epochs,lr,latent_dim):\n    loss=nn.BCELoss()\n    Tensor=torch.cuda.FloatTensor\n    trainer_D=torch.optim.Adam(net_D.parameters(),lr=lr,betas=(0.5,0.999))\n    trainer_G=torch.optim.Adam(net_G.parameters(),lr=lr,betas=(0.5,0.999))\n    plt.figure(figsize=(7,4))\n    d_loss_point=[]\n    g_loss_point=[]\n    d_loss=0\n    g_loss=0\n    for epoch in range(1,num_epochs+1):\n        d_loss_sum=0\n        g_loss_sum=0\n        batch=0\n        for X in data_iter:\n            X=X[:][0]\n            batch+=1\n            X=Variable(X.type(Tensor))\n            batch_size=X.shape[0]\n            Z=Variable(Tensor(np.random.normal(0,1,(batch_size,latent_dim,1,1))))\n\n            trainer_D.zero_grad()\n            d_loss = update_D(X, Z, net_D, net_G, loss, trainer_D)\n            d_loss_sum+=d_loss\n            trainer_G.zero_grad()\n            g_loss = update_G(Z, net_D, net_G, loss, trainer_G)\n            g_loss_sum+=g_loss\n\n        d_loss_point.append(d_loss_sum/batch)\n        g_loss_point.append(g_loss_sum/batch)\n        print(\n            \"[Epoch %d/%d]  [D loss: %f] [G loss: %f]\"\n            % (epoch, num_epochs,  d_loss_sum/batch_size,  g_loss_sum/batch_size)\n        )\n\n\n    plt.ylabel('Loss', fontdict={ 'size': 14})\n    plt.xlabel('epoch', fontdict={ 'size': 14})\n    plt.xticks(range(0,num_epochs+1,3))\n    plt.plot(range(1,num_epochs+1),d_loss_point,color='orange',label='discriminator')\n    plt.plot(range(1,num_epochs+1),g_loss_point,color='blue',label='generator')\n    plt.legend()\n    plt.show()\n    print(d_loss,g_loss)\n\n    Z = Variable(Tensor(np.random.normal(0, 1, size=(21, latent_dim, 1, 1))),requires_grad=False)\n    fake_x = generator(Z)\n    fake_x=fake_x.cpu().detach().numpy()\n    plt.figure(figsize=(14,6))\n    for i in range(21):\n        im=np.transpose(fake_x[i])\n        plt.subplot(3,7,i+1)\n        plt.imshow(im)\n    plt.show()","execution_count":null},{"metadata":{"id":"F8C60647DEC44AE9ACFC93356AC83AD2","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"if __name__ == '__main__':\n    lr,latent_dim,num_epochs=0.005,100,50\n    train(discriminator,generator,data_iter,num_epochs,lr,latent_dim)","execution_count":null},{"metadata":{"id":"BA65318A11604C428E43E527B91EE8B1","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"","execution_count":null}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.3","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":0}